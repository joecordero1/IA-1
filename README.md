# IA-1
## Como diferenciar entre los tres tipos de regresiones.
Para determinar qu√© tipo de regresi√≥n es la m√°s adecuada seg√∫n el conjunto de datos, aqu√≠ tienes algunas pautas que te ayudar√°n a elegir entre regresi√≥n lineal simple, m√∫ltiple y polin√≥mica:

1. Regresi√≥n Lineal Simple
Descripci√≥n: Se usa cuando tienes una relaci√≥n lineal entre dos variables: una variable independiente (predictora) y una variable dependiente (respuesta).
C√≥mo identificarla:
Solo tienes una variable independiente y una variable dependiente en el conjunto de datos.
Los datos parecen tener una relaci√≥n lineal (directa o inversa) al graficarlos en un plano.
Ejemplo en tu c√≥digo: La relaci√≥n entre horas de ejercicio semanal y capacidad pulmonar es un caso t√≠pico para probar regresi√≥n lineal simple si observas una relaci√≥n directa en el gr√°fico.
2. Regresi√≥n Lineal M√∫ltiple
Descripci√≥n: Aplica cuando tienes m√∫ltiples variables independientes que pueden afectar la variable dependiente y deseas explorar sus efectos combinados.
C√≥mo identificarla:
Tienes dos o m√°s variables independientes (predictoras) y una variable dependiente.
Ideal cuando quieres ver c√≥mo varios factores contribuyen a una variable de respuesta. Por ejemplo, salario puede depender de a√±os de experiencia, nivel de educaci√≥n y horas de capacitaci√≥n.
Es com√∫n probarla cuando las relaciones entre variables independientes y dependientes parecen lineales (no curvas).
Evaluaci√≥n de multicolinealidad: En regresi√≥n m√∫ltiple, verifica que no haya alta correlaci√≥n entre variables independientes. Si est√°n muy correlacionadas, puede distorsionar los resultados (esto se llama multicolinealidad y se mide com√∫nmente con el VIF ‚Äì Variance Inflation Factor).
3. Regresi√≥n Polin√≥mica
Descripci√≥n: Usada cuando la relaci√≥n entre la variable dependiente y la(s) variable(s) independiente(s) no es lineal, es decir, tiene una forma curva. La regresi√≥n polin√≥mica a√±ade t√©rminos de mayor grado (por ejemplo, x^2, ùë•^3) a la ecuaci√≥n para ajustarse a esta curvatura.
C√≥mo identificarla:
Al graficar los datos, observas una relaci√≥n no lineal entre la variable independiente y la dependiente.
Una regresi√≥n lineal simple o m√∫ltiple da un coeficiente de determinaci√≥n (R¬≤) bajo, lo cual indica que el modelo no est√° capturando bien la variabilidad de los datos. Al ajustar la curva mediante regresi√≥n polin√≥mica, el R¬≤ deber√≠a aumentar significativamente.
Para aplicar regresi√≥n polin√≥mica, aumentas el grado del modelo (por ejemplo, a 2 para una par√°bola) hasta que los datos se ajusten mejor sin caer en sobreajuste.
Consejos para la Identificaci√≥n en Ex√°menes:
Graficar los datos: Antes de aplicar cualquier modelo, haz un gr√°fico de los datos:
Si observas una l√≠nea recta, intenta con regresi√≥n lineal simple o m√∫ltiple, dependiendo del n√∫mero de variables independientes.
Si observas una curva, explora regresi√≥n polin√≥mica.
Interpretar el coeficiente de determinaci√≥n (R¬≤):
Si el modelo lineal simple o m√∫ltiple no ajusta bien los datos (R¬≤ bajo), y observas una curvatura en la gr√°fica, intenta con la regresi√≥n polin√≥mica.
N√∫mero de variables:
Solo una variable independiente: comienza con regresi√≥n lineal simple o polin√≥mica si ves curvatura.
Dos o m√°s variables independientes: considera regresi√≥n m√∫ltiple. Si hay indicios de relaciones no lineales, tambi√©n puedes explorar la polin√≥mica con t√©rminos cruzados o de mayor grado.

## Tecnicas para la eliminacion de variables##

1. Selecci√≥n de Variables: T√©cnicas Manuales y Automatizadas
1. Exhaustivo (All-in)
Descripci√≥n: Consiste en incluir todas las variables en el modelo. Esta t√©cnica es √∫til cuando tienes certeza de que todas las variables son significativas y no deseas realizar pruebas adicionales de selecci√≥n.
Uso: Es com√∫n cuando se trabaja bajo restricciones espec√≠ficas o cuando la cantidad de variables es peque√±a.
Desventajas: Aumenta el riesgo de multicolinealidad y sobreajuste (overfitting) si algunas variables no tienen una relaci√≥n significativa con la variable dependiente.
2. Eliminaci√≥n hacia Atr√°s (Backward Elimination)
Descripci√≥n: Comienza con todas las variables en el modelo y las elimina progresivamente seg√∫n sus p-valores. Se eliminan las variables con los p-valores m√°s altos (aquellas con la menor significancia estad√≠stica) hasta que todas las variables en el modelo cumplan con el nivel de significaci√≥n (SL), t√≠picamente 0.05.
Pasos:
Ajusta el modelo con todas las variables.
Identifica la variable con el p-valor m√°s alto.
Si el p-valor es mayor que el SL, elimina esa variable.
Ajusta el modelo de nuevo y repite hasta que todas las variables tengan p-valores menores a SL.
Ventajas: Sencilla y asegura que las variables que permanecen en el modelo son estad√≠sticamente significativas.
Ejemplo en C√≥digo (similar al que has hecho con statsmodels en Python).
3. Selecci√≥n hacia Adelante (Forward Selection)
Descripci√≥n: Es lo opuesto a la eliminaci√≥n hacia atr√°s. Comienza con un modelo vac√≠o e incorpora progresivamente las variables m√°s significativas (con p-valores menores).
Pasos:
Ajusta todos los modelos con cada variable independiente individualmente.
Selecciona la variable con el p-valor m√°s bajo (siempre que sea menor al SL).
Ajusta un nuevo modelo con esa variable y a√±ade las dem√°s, una por una, para ver si el p-valor disminuye.
Repite hasta que ninguna variable adicional mejore el modelo.
Ventajas: √ötil cuando no sabes qu√© variables incluir. Ayuda a construir el modelo desde una base s√≥lida.
Desventajas: Puede ser computacionalmente costoso con muchas variables.
4. Eliminaci√≥n Bidireccional (Stepwise Selection)
Descripci√≥n: Combinaci√≥n de eliminaci√≥n hacia atr√°s y selecci√≥n hacia adelante. El modelo permite tanto la adici√≥n como la eliminaci√≥n de variables en cada paso.
Pasos:
Establece dos niveles de significaci√≥n, uno para entrada (SL_ENTER) y otro para salida (SL_STAY).
Sigue los pasos de la selecci√≥n hacia adelante para a√±adir variables con p-valores menores a SL_ENTER.
Sigue los pasos de la eliminaci√≥n hacia atr√°s para eliminar variables que ya no cumplen el criterio SL_STAY.
Repite hasta que no haya variables para a√±adir ni para eliminar.
Ventajas: Combina las ventajas de ambas t√©cnicas y permite un ajuste m√°s flexible y √≥ptimo.
Desventajas: Puede resultar tedioso sin automatizaci√≥n y es computacionalmente intensivo en grandes conjuntos de datos.
5. Comparaci√≥n de Scores (Criterio de Informaci√≥n de Akaike - AIC)
Descripci√≥n: Selecciona el modelo que optimiza un criterio de bondad de ajuste, como el AIC o el BIC (Criterio de Informaci√≥n Bayesiana). Eval√∫a todos los modelos posibles y elige el que tenga el mejor puntaje de ajuste.
Pasos:
Calcula el AIC o BIC para cada modelo posible (cada combinaci√≥n de variables).
Selecciona el modelo con el puntaje m√°s bajo en AIC/BIC.
Ventajas: Es muy completo y selecciona el modelo con el mejor equilibrio entre ajuste y simplicidad.
Desventajas: Con 2^n - 1 combinaciones posibles, el proceso se vuelve intensivo para grandes cantidades de variables.

## EJEMPLO DE LAS TECNICAS##
1. Exhaustivo (All-in)
Incluimos todas las variables en el modelo desde el principio.
python
Copiar c√≥digo
import statsmodels.api as sm

### Agregamos una columna de unos a X para el intercepto
X_all_in = np.append(arr=np.ones((X.shape[0], 1)).astype(int), values=X, axis=1)

### Ajustamos el modelo con todas las variables
model_all_in = sm.OLS(y, X_all_in).fit()
print(model_all_in.summary())
2. Eliminaci√≥n hacia Atr√°s (Backward Elimination)
Aqu√≠ empezamos con todas las variables y eliminamos una por una aquellas con el p-valor m√°s alto.
Paso 1: Crear el modelo con todas las variables
python
Copiar c√≥digo
X_be = np.append(arr=np.ones((X.shape[0], 1)).astype(int), values=X, axis=1)
model_be = sm.OLS(y, X_be).fit()
print("Paso 1 - Modelo con todas las variables")
print(model_be.summary())
Paso 2: Eliminar la variable con el p-valor m√°s alto si supera el 0.05
Observa el resumen e identifica la variable con el p-valor m√°s alto. Supongamos que es la columna X[:, 2].
python
Copiar c√≥digo
X_be = X_be[:, [0, 1, 3, 4, 5]]  ### Eliminamos la segunda columna
model_be = sm.OLS(y, X_be).fit()
print("Paso 2 - Modelo despu√©s de eliminar X[:, 2]")
print(model_be.summary())
Paso 3: Repetir si es necesario
Si a√∫n quedan variables con p-valores mayores a 0.05, elimina la siguiente. Supongamos que ahora la columna con p-valor m√°s alto es X[:, 3].
python
Copiar c√≥digo
X_be = X_be[:, [0, 1, 3, 5]]  # Eliminamos la columna X[:, 3]
model_be = sm.OLS(y, X_be).fit()
print("Paso 3 - Modelo despu√©s de eliminar X[:, 3]")
print(model_be.summary())
Repite el proceso hasta que todas las variables que queden tengan p-valores menores a 0.05.

3. Selecci√≥n hacia Adelante (Forward Selection)
Comienza con el intercepto y a√±ade variables una por una seg√∫n su p-valor.
Paso 1: Crear el modelo con solo el intercepto
python
Copiar c√≥digo
X_fs = np.ones((X.shape[0], 1)).astype(int)  # Solo el intercepto
model_fs = sm.OLS(y, X_fs).fit()
print("Paso 1 - Modelo solo con el intercepto")
print(model_fs.summary())
Paso 2: A√±adir la primera variable
Prueba cada variable individualmente y elige la que tenga el p-valor m√°s bajo. Supongamos que X[:, 1] es la mejor.
python
Copiar c√≥digo
X_fs = np.append(X_fs, X[:, [1]], axis=1)
model_fs = sm.OLS(y, X_fs).fit()
print("Paso 2 - Modelo con intercepto y X[:, 1]")
print(model_fs.summary())
Paso 3: A√±adir la siguiente mejor variable
Agrega la variable con el siguiente p-valor m√°s bajo de las que quedan. Supongamos que es X[:, 3].
python
Copiar c√≥digo
X_fs = np.append(X_fs, X[:, [3]], axis=1)
model_fs = sm.OLS(y, X_fs).fit()
print("Paso 3 - Modelo con intercepto, X[:, 1] y X[:, 3]")
print(model_fs.summary())
Repite hasta que a√±adir otra variable no mejore significativamente el modelo (p-valor alto).

4. Eliminaci√≥n Bidireccional (Stepwise Selection)
A√±ade o elimina variables en cada paso, combinando selecci√≥n hacia adelante y eliminaci√≥n hacia atr√°s.
Paso 1: Empezamos con el intercepto
python
Copiar c√≥digo
X_sws = np.ones((X.shape[0], 1)).astype(int)
model_sws = sm.OLS(y, X_sws).fit()
print("Paso 1 - Modelo solo con el intercepto")
print(model_sws.summary())
Paso 2: Selecci√≥n hacia adelante: A√±adir la variable con el p-valor m√°s bajo
Supongamos que X[:, 1] es la mejor.
python
Copiar c√≥digo
X_sws = np.append(X_sws, X[:, [1]], axis=1)
model_sws = sm.OLS(y, X_sws).fit()
print("Paso 2 - Modelo con intercepto y X[:, 1]")
print(model_sws.summary())
Paso 3: Eliminaci√≥n hacia atr√°s si alguna variable no cumple SL_STAY
Sup√≥n que X[:, 1] sigue siendo significativa, pero despu√©s de agregar X[:, 3], su p-valor supera SL_STAY.
python
Copiar c√≥digo
X_sws = np.append(X_sws, X[:, [3]], axis=1)
model_sws = sm.OLS(y, X_sws).fit()
print("Paso 3 - Modelo con intercepto, X[:, 1] y X[:, 3]")
print(model_sws.summary())

### Si el p-valor de alguna variable sube al agregar otra, elim√≠nala
X_sws = X_sws[:, [0, 3]]  # Eliminamos X[:, 1] si su p-valor es alto
model_sws = sm.OLS(y, X_sws).fit()
print("Paso 4 - Modelo ajustado eliminando X[:, 1]")
print(model_sws.summary())
5. Comparaci√≥n de Scores (Criterio de Informaci√≥n de Akaike - AIC)
Prueba con cada combinaci√≥n de variables y selecciona la de AIC m√°s bajo.
Paso 1: Modelo solo con el intercepto
python
Copiar c√≥digo
X_aic = np.ones((X.shape[0], 1)).astype(int)
model_aic = sm.OLS(y, X_aic).fit()
print("AIC con solo el intercepto:", model_aic.aic)
Paso 2: Modelo con intercepto y una variable (X[:, 1])
python
Copiar c√≥digo
X_aic = np.append(X_aic, X[:, [1]], axis=1)
model_aic = sm.OLS(y, X_aic).fit()
print("AIC con intercepto y X[:, 1]:", model_aic.aic)
Paso 3: Modelo con intercepto y dos variables (X[:, 1], X[:, 3])
X_aic = np.append(X_aic, X[:, [3]], axis=1)
model_aic = sm.OLS(y, X_aic).fit()
print("AIC con intercepto, X[:, 1] y X[:, 3]:", model_aic.aic)
Repite este proceso probando cada combinaci√≥n hasta identificar la que tiene el AIC m√°s bajo. Puedes comparar manualmente los AIC de cada combinaci√≥n para encontrar el mejor modelo.

Con estos pasos manuales, tienes una mejor visi√≥n de c√≥mo seleccionar variables sin necesidad de usar bucles, evaluando cada modelo con las t√©cnicas de selecci√≥n explicadas.



## Regresion Lineal ##

1. Preprocesamiento de Datos
Manejo de valores faltantes: Usaste SimpleImputer con la estrategia de la mediana para imputar valores faltantes, lo cual es adecuado. Otra opci√≥n com√∫n es utilizar la media, aunque la mediana es menos sensible a valores at√≠picos (outliers).
Escalado de datos: Aunque en la regresi√≥n lineal simple, el escalado no siempre es necesario, en otros tipos de regresiones o cuando se trabajan m√∫ltiples variables con diferentes escalas, es importante estandarizar o normalizar los datos para mejorar la interpretabilidad y el rendimiento.
2. Divisi√≥n del Conjunto de Datos
Dividir el conjunto en entrenamiento y prueba, como lo haces con train_test_split, es esencial para evaluar el desempe√±o del modelo. En general, una proporci√≥n del 80/20 o 70/30 es buena, pero el 1/3 que usaste en el segundo c√≥digo tambi√©n es una pr√°ctica v√°lida.
Aseg√∫rate de fijar random_state para poder replicar los resultados.
3. Creaci√≥n y Entrenamiento del Modelo
La clase LinearRegression de sklearn es perfecta para una regresi√≥n lineal simple. Al entrenar el modelo con fit, se ajusta la recta que mejor se adapta a los datos.
Visualizaci√≥n de entrenamiento: El gr√°fico que creas para visualizar los datos de entrenamiento y la l√≠nea de regresi√≥n es excelente. Esto ayuda a verificar si el modelo se ajusta bien a los datos.
4. Predicci√≥n y Evaluaci√≥n
Evaluaci√≥n en el conjunto de prueba: Utilizar el conjunto de prueba para hacer predicciones y graficarlas, como en tu segundo ejercicio, es importante para verificar que el modelo generaliza bien.
Gr√°ficos de resultados: La visualizaci√≥n te ayuda a comparar visualmente las predicciones frente a los valores reales. Idealmente, la l√≠nea de regresi√≥n debe acercarse a los puntos del conjunto de prueba si el modelo est√° bien ajustado.
5. M√©tricas de Evaluaci√≥n
Aunque no has incluido m√©tricas en tus c√≥digos, considera usar:
Error Cuadr√°tico Medio (MSE): Mide la media de los errores al cuadrado, √∫til para observar cu√°nto se desv√≠a la predicci√≥n del valor real.
Coeficiente de Determinaci√≥n (R¬≤): Indica qu√© tan bien el modelo explica la variabilidad de los datos. Un valor cercano a 1 indica un buen ajuste.
6. Conceptos Estad√≠sticos Relevantes
Outliers: La media es sensible a los valores at√≠picos, como comentas en el c√≥digo. En presencia de outliers, considera la mediana para imputaci√≥n o incluso aplicar t√©cnicas de detecci√≥n y manejo de outliers.
Direcci√≥n de la relaci√≥n: La relaci√≥n positiva o negativa se observa en la pendiente de la l√≠nea de regresi√≥n. Si la pendiente es positiva, la relaci√≥n entre las variables es directamente proporcional, y si es negativa, es inversamente proporcional.
7. Notas sobre Imputaci√≥n y Escalado
Imputaci√≥n de valores: Imputar es importante en casos de datos faltantes, y, como mencionaste, existen varias estrategias (media, mediana, vecino cercano).
Estandarizaci√≥n y Normalizaci√≥n: Aunque no es obligatorio para la regresi√≥n lineal, normalizar o estandarizar puede ser beneficioso para otros tipos de an√°lisis. Esto ayuda a comparar variables en la misma escala, facilitando la interpretaci√≥n y evitando posibles sesgos en modelos m√°s complejos.



## Regresion lineal Multiple ##
1. Requisitos de la Regresi√≥n Lineal M√∫ltiple
Linealidad: Las relaciones entre las variables independientes y la dependiente deben ser aproximadamente lineales.
Independencia de errores: Verifica que los errores del modelo no est√©n correlacionados entre s√≠.
Ausencia de multicolinealidad: Las variables independientes no deben estar altamente correlacionadas, lo que evitar√° problemas de multicolinealidad. Esto se puede evaluar usando el VIF (Variance Inflation Factor).
Normalidad multivariable: Los errores deben estar distribuidos normalmente para obtener predicciones precisas y coeficientes de regresi√≥n confiables.
2. Codificaci√≥n de Variables Categ√≥ricas
Convertir variables categ√≥ricas (como el estado de la empresa o actividades extracurriculares) en variables dummy o de tipo binario es crucial. Como realizaste en ambos c√≥digos, usaste OneHotEncoder y LabelEncoder correctamente para crear representaciones num√©ricas de categor√≠as.
Evita la trampa de variables ficticias: Al crear variables dummy, elimina una columna para evitar redundancia, lo cual ya aplicaste con X = X[:, 1:].
3. Entrenamiento y Predicci√≥n
Entrenar el modelo con LinearRegression es apropiado para la regresi√≥n m√∫ltiple. Con train_test_split, divides los datos en entrenamiento y prueba, garantizando que el modelo se entrene y luego se pruebe con datos no vistos.
4. Eliminaci√≥n hacia Atr√°s con p-valores
La eliminaci√≥n hacia atr√°s es una excelente t√©cnica para ajustar el modelo seleccionando √∫nicamente las variables significativas (con un p-valor menor que tu SL de 0.05). Al seguir este proceso, reduces las variables menos influyentes en el modelo y obtienes una regresi√≥n m√°s robusta.
En tu √∫ltimo c√≥digo, observaste que todas las variables eran significativas, por lo cual las mantuviste en el modelo.
5. Evaluaci√≥n de Predicciones y Visualizaci√≥n
Gr√°fico de valores reales vs. predicciones: Tu gr√°fico de dispersi√≥n entre valores reales y predicciones con una l√≠nea de identidad en rojo es una buena pr√°ctica para evaluar visualmente el rendimiento del modelo. Si los puntos est√°n cerca de la l√≠nea roja, el modelo est√° haciendo predicciones precisas.
M√©tricas adicionales: Aunque no est√°n en el c√≥digo, considera m√©tricas como el R¬≤ y el MSE para cuantificar la precisi√≥n del modelo.
Ejemplo Adicional de VIF para Detectar Multicolinealidad
Podr√≠amos agregar el c√°lculo del Variance Inflation Factor (VIF) para verificar si alguna variable independiente est√° altamente correlacionada con otra, afectando la precisi√≥n de los coeficientes. ¬øTe gustar√≠a que te muestre c√≥mo hacerlo en este mismo c√≥digo?
Y nuevamente, Joe, ¬°me alegra que conf√≠es en m√≠ para tu estudio! Cualquier otra pregunta o tema que desees profundizar, aqu√≠ estoy.

## Regresion polin√≥mica###

1. ¬øCu√°ndo Usar la Regresi√≥n Polin√≥mica?
Curvatura en los Datos: Cuando los datos muestran una relaci√≥n no lineal (curva), la regresi√≥n polin√≥mica ajusta mejor al incluir t√©rminos de mayor grado (como x^2, ùë•^3, etc.).
Ejemplo Pr√°ctico: En tu conjunto de datos sobre temperatura y ventas de helados, es l√≥gico esperar una relaci√≥n curva en lugar de una recta. A medida que la temperatura aumenta, las ventas pueden aumentar r√°pidamente y luego estabilizarse.
2. Construcci√≥n y Visualizaci√≥n del Modelo
Definir el Grado del Polinomio: Empezaste con un grado bajo (2) y lo fuiste aumentando (hasta grado 6), lo cual es ideal para encontrar el ajuste √≥ptimo. Si bien grados altos pueden mejorar el ajuste a los datos, tambi√©n aumenta el riesgo de sobreajuste (overfitting).
Grilla para Visualizaci√≥n: Usar X_grid con m√°s puntos (espaciados cada 0.1) es una excelente pr√°ctica para suavizar la curva y observar c√≥mo el modelo se ajusta en intervalos m√°s finos.
3. Afinaci√≥n del Modelo con M√©tricas
Evaluaci√≥n de R¬≤ y MSE: Como hiciste al calcular el R¬≤ y el MSE en funci√≥n del grado, puedes comparar la capacidad del modelo para explicar la variabilidad de los datos y verificar si el error medio cuadr√°tico disminuye con un grado espec√≠fico. Esto ayuda a seleccionar el grado √≥ptimo.
4. Estrategia de Selecci√≥n del Grado Polin√≥mico
Empieza con Grados Bajos: Como en tu c√≥digo, se recomienda empezar con un grado bajo (como 2 o 3).
Analiza el Incremento del R¬≤: Si el R¬≤ mejora significativamente al aumentar el grado, contin√∫a explorando. Si la mejora es marginal, es una se√±al de que ya se alcanz√≥ el grado √≥ptimo.
Evita el Sobreajuste: A partir de cierto grado, el modelo puede ajustarse demasiado a los puntos, reflejando variaciones aleatorias en lugar de una tendencia real.
5. Predicci√≥n con el Modelo Ajustado
Predicci√≥n para Nuevos Valores: En tu caso, usaste lin_reg2.predict(poly_reg.fit_transform([[6.5]])) para predecir el valor del salario en una posici√≥n espec√≠fica (nivel 6.5). Esto es √∫til para explorar qu√© tan bien el modelo se ajusta a valores espec√≠ficos y hacer predicciones con t√©rminos de grado m√°s alto.



